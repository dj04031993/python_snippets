import json
import requests
import concurrent.futures
import time
from datetime import datetime

def post_request(payload, endpoint_url, headers):
    """
    Make a POST request to the ATP Check endpoint
    
    Args:
        payload (dict): JSON payload to send
        endpoint_url (str): API endpoint URL
        headers (dict): Request headers
        
    Returns:
        tuple: (payload, response_dict, status_code)
    """
    try:
        response = requests.post(endpoint_url, json=payload, headers=headers)
        status_code = response.status_code
        
        if status_code == 200:
            return payload, response.json(), status_code
        else:
            return payload, {"error": f"Request failed with status code {status_code}"}, status_code
            
    except Exception as e:
        return payload, {"error": str(e)}, 0

def process_atp_checks(input_file, output_file, endpoint_url, max_workers=10, batch_size=100):
    """
    Process ATP check requests in parallel and save responses
    
    Args:
        input_file (str): Path to input file with JSON payloads (one per line)
        output_file (str): Path to output file for saving responses
        endpoint_url (str): API endpoint URL
        max_workers (int): Maximum number of parallel workers
        batch_size (int): Number of requests to process in each batch
    """
    # Headers for API request
    headers = {
        'Content-Type': 'application/json',
        # Add your authorization header here if needed
        # 'Authorization': 'Bearer YOUR_TOKEN'
    }
    
    start_time = time.time()
    total_processed = 0
    total_success = 0
    total_failed = 0
    
    print(f"Starting to process ATP check requests at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:
        # Read all lines first
        lines = f_in.readlines()
        total_lines = len(lines)
        
        print(f"Loaded {total_lines} payloads from {input_file}")
        
        # Process in batches to avoid memory issues with large files
        for i in range(0, total_lines, batch_size):
            batch = lines[i:i+batch_size]
            batch_payloads = []
            
            # Parse JSON payloads from the batch
            for line in batch:
                try:
                    payload = json.loads(line.strip())
                    batch_payloads.append(payload)
                except json.JSONDecodeError:
                    print(f"Warning: Invalid JSON at line {i+batch.index(line)+1}, skipping")
                    total_failed += 1
            
            # Process the batch in parallel
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [
                    executor.submit(post_request, payload, endpoint_url, headers)
                    for payload in batch_payloads
                ]
                
                for future in concurrent.futures.as_completed(futures):
                    original_payload, response_data, status_code = future.result()
                    
                    # Combine original payload with the response for context
                    result = {
                        "requestPayload": original_payload,
                        "response": response_data,
                        "statusCode": status_code,
                        "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    # Write the result to the output file
                    f_out.write(json.dumps(result) + '\n')
                    f_out.flush()  # Ensure data is written immediately
                    
                    # Update counters
                    total_processed += 1
                    if status_code == 200:
                        total_success += 1
                    else:
                        total_failed += 1
            
            # Print progress after each batch
            progress = (i + len(batch)) / total_lines * 100
            elapsed = time.time() - start_time
            print(f"Progress: {progress:.2f}% ({i + len(batch)}/{total_lines}) | "
                  f"Elapsed: {elapsed:.2f}s | Success: {total_success} | Failed: {total_failed}")
    
    # Print final summary
    total_time = time.time() - start_time
    print(f"\nSummary:")
    print(f"Total processed: {total_processed}")
    print(f"Successful: {total_success}")
    print(f"Failed: {total_failed}")
    print(f"Total time: {total_time:.2f} seconds")
    print(f"Average time per request: {(total_time/total_processed if total_processed else 0):.4f} seconds")
    print(f"Requests per second: {(total_processed/total_time if total_time else 0):.2f}")
    print(f"Results saved to {output_file}")

if __name__ == "__main__":
    # Configuration settings
    INPUT_FILE = "atp_payloads.txt"  # File containing JSON payloads, one per line
    OUTPUT_FILE = f"atp_responses_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"  # Output file with timestamp
    ENDPOINT_URL = ""
    MAX_WORKERS = 20  # Adjust based on your system capabilities and API rate limits
    BATCH_SIZE = 500  # Process 500 requests at a time
    
    # Run the processor
    process_atp_checks(INPUT_FILE, OUTPUT_FILE, ENDPOINT_URL, MAX_WORKERS, BATCH_SIZE)
